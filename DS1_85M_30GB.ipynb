{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTmTCRXiNqgh",
        "outputId": "87a02404-e172-4b3f-d0e4-7e89dc0202b7"
      },
      "outputs": [],
      "source": [
        "!wget \"https://object.pouta.csc.fi/OPUS-OpenSubtitles/v2024/mono/en.txt.gz\"\n",
        "!gunzip en.txt.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hiIv3bhOc3RU",
        "outputId": "28a24017-ed03-406b-b3a9-1d0c29eb2e3b"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import pipeline\n",
        "import os\n",
        "import glob\n",
        "import shutil\n",
        "\n",
        "\n",
        "os.makedirs('/content/split_files', exist_ok=True)\n",
        "os.makedirs('/content/translated', exist_ok=True)\n",
        "# Drive folder for checkpoints\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "os.makedirs('/content/drive/MyDrive/translation_checkpoints', exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-WAnx2wz7XT",
        "outputId": "2fad5fce-710a-4b67-a713-5f1481dd4892"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "def add_translated_splits_to_log(translation_dir='/content/translated'):\n",
        "    # Log file where completed splits are stored\n",
        "    completed_log = '/content/translated/log_completed.txt'\n",
        "\n",
        "    # Get the list of already translated split files in the translation directory\n",
        "    translated_files = [f for f in os.listdir(translation_dir) if f.endswith('_translated.csv')]\n",
        "\n",
        "    # Extract base names from the translated files (remove the '_translated.csv' part)\n",
        "    translated_splits = {os.path.splitext(f)[0].replace('DS4', '') for f in translated_files}\n",
        "\n",
        "    # Read the existing log file to get already recorded completed splits\n",
        "    if os.path.exists(completed_log):\n",
        "        with open(completed_log, 'r') as log_file:\n",
        "            already_logged_splits = {line.strip() for line in log_file.readlines()}\n",
        "    else:\n",
        "        already_logged_splits = set()\n",
        "\n",
        "    # Combine the already logged splits with the new translated ones\n",
        "    new_splits_to_add = translated_splits - already_logged_splits\n",
        "\n",
        "    # If there are any new splits to add, append them to the log file\n",
        "    if new_splits_to_add:\n",
        "        with open(completed_log, 'a') as log_file:\n",
        "            for split in new_splits_to_add:\n",
        "                log_file.write(f\"{split}\\n\")\n",
        "        print(f\"‚úÖ Added {len(new_splits_to_add)} new splits to the log.\")\n",
        "    else:\n",
        "        print(\"No new splits to add to the log.\")\n",
        "\n",
        "# Run this function to add the already translated splits to the log file\n",
        "add_translated_splits_to_log()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yEHdaiXLczxL",
        "outputId": "382b45a8-d427-4894-a55c-f884af648fd3"
      },
      "outputs": [],
      "source": [
        "\n",
        "language_models = {\n",
        "    'Chinese': 'Helsinki-NLP/opus-mt-en-zh',\n",
        "    'Spanish': 'Helsinki-NLP/opus-mt-en-es',\n",
        "    'French': 'Helsinki-NLP/opus-mt-en-fr',\n",
        "    'German': 'Helsinki-NLP/opus-mt-en-de',\n",
        "    'Italian': 'Helsinki-NLP/opus-mt-en-it',\n",
        "    'Hindi': 'Helsinki-NLP/opus-mt-en-hi',\n",
        "    'Urdu': 'Helsinki-NLP/opus-mt-en-ur',\n",
        "    'Arabic': 'Helsinki-NLP/opus-mt-en-ar',\n",
        "}\n",
        "\n",
        "# Determine if GPU is available for faster processing\n",
        "device = 0 if torch.cuda.is_available() else -1  # Use GPU if available\n",
        "translation_pipelines = {}\n",
        "\n",
        "print(\"üîµ Loading translation models...\")\n",
        "for lang, model_name in language_models.items():\n",
        "    translation_pipelines[lang] = pipeline('translation', model=model_name, tokenizer=model_name, device=device)\n",
        "print(\"‚úÖ Models loaded successfully!\")\n",
        "\n",
        "def translate_line(text):\n",
        "    translations = {'English': text}\n",
        "    for lang, translator in translation_pipelines.items():\n",
        "        try:\n",
        "            translated = translator(text, max_length=512, truncation=True)[0]['translation_text']\n",
        "        except Exception as e:\n",
        "            print(f\"Error translating '{text[:30]}...' to {lang}: {e}\")\n",
        "            translated = \"\"\n",
        "        translations[lang] = translated\n",
        "    return translations\n",
        "\n",
        "def split_file(input_file, lines_per_file=100000):\n",
        "    with open(input_file, 'r', encoding='utf-8') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    lines = [line.strip() for line in lines if line.strip()]\n",
        "    total_parts = (len(lines) + lines_per_file - 1) // lines_per_file\n",
        "\n",
        "    for part_num in range(total_parts):\n",
        "        part_lines = lines[part_num * lines_per_file : (part_num + 1) * lines_per_file]\n",
        "        part_filename = f'/content/split_files/DS1split_part_{part_num+1}.txt'\n",
        "        with open(part_filename, 'w', encoding='utf-8') as part_file:\n",
        "            part_file.write('\\n'.join(part_lines))\n",
        "        print(f\"‚úÖ Saved split: {part_filename} ({len(part_lines)} lines)\")\n",
        "\n",
        "def log_completed_split(split_name):\n",
        "    completed_log = '/content/translated/log_completed.txt'\n",
        "\n",
        "    with open(completed_log, 'a') as f:\n",
        "        f.write(split_name + '\\n')\n",
        "    print(f\"‚úÖ Logged completed split: {split_name}\")\n",
        "\n",
        "def translate_file(split_file_path, batch_size=100):\n",
        "    base_name = os.path.basename(split_file_path).replace('.txt', '')\n",
        "    checkpoint_csv_path = f'/content/drive/MyDrive/translation_checkpoints/{base_name}_checkpoint.csv'\n",
        "    output_csv_path = f'/content/translated/DS1{base_name}_translated.csv'\n",
        "\n",
        "    # Check if already translated (based on output CSV)\n",
        "    if os.path.exists(output_csv_path):\n",
        "        print(f\"‚è© Skipping {split_file_path} ‚Äî already translated.\")\n",
        "        os.remove(split_file_path)  # Optional cleanup\n",
        "        return\n",
        "\n",
        "    # Create a log file to track completed translations\n",
        "    completed_log = '/content/translated/log_completed.txt'\n",
        "    if not os.path.exists(completed_log):\n",
        "        open(completed_log, 'w').close()\n",
        "\n",
        "    with open(completed_log, 'r') as f:\n",
        "        completed_parts = set(line.strip() for line in f.readlines())\n",
        "\n",
        "    # Skip if already completed (check log)\n",
        "    if base_name in completed_parts:\n",
        "        print(f\"‚è© Skipping {split_file_path} ‚Äî already completed.\")\n",
        "        os.remove(split_file_path)  # Optional cleanup\n",
        "        return\n",
        "\n",
        "    with open(split_file_path, 'r', encoding='utf-8') as f:\n",
        "        lines = f.readlines()\n",
        "    lines = [line.strip() for line in lines if line.strip()]\n",
        "\n",
        "    # Check if resuming from checkpoint\n",
        "    if os.path.exists(checkpoint_csv_path):\n",
        "        translated_df = pd.read_csv(checkpoint_csv_path)\n",
        "        start_from_line = len(translated_df)\n",
        "        print(f\"üîÅ Resuming from line {start_from_line}\")\n",
        "    else:\n",
        "        translated_df = pd.DataFrame()\n",
        "        start_from_line = 0\n",
        "\n",
        "    for batch_start in range(start_from_line, len(lines), batch_size):\n",
        "        batch = lines[batch_start : batch_start + batch_size]\n",
        "        batch_translations = []\n",
        "\n",
        "        for line in batch:\n",
        "            result = translate_line(line)\n",
        "            batch_translations.append(result)\n",
        "\n",
        "        batch_df = pd.DataFrame(batch_translations)\n",
        "        translated_df = pd.concat([translated_df, batch_df], ignore_index=True)\n",
        "\n",
        "        # Save checkpoint after each batch\n",
        "        translated_df.to_csv(checkpoint_csv_path, index=False, encoding='utf-8-sig')\n",
        "        print(f\"‚úÖ Translated {len(translated_df)} / {len(lines)} lines for {split_file_path}\")\n",
        "\n",
        "    # Final save\n",
        "    translated_df.to_csv(output_csv_path, index=False, encoding='utf-8-sig')\n",
        "    print(f\"üéØ Finished translation: {output_csv_path}\")\n",
        "\n",
        "    # Clean up files\n",
        "    os.remove(split_file_path)\n",
        "    print(f\"üóëÔ∏è Deleted split file to save space: {split_file_path}\")\n",
        "\n",
        "    # Log completion of this part\n",
        "    log_completed_split(base_name)\n",
        "\n",
        "    if os.path.exists(checkpoint_csv_path):\n",
        "        os.remove(checkpoint_csv_path)\n",
        "        print(f\"üóëÔ∏è Deleted checkpoint: {checkpoint_csv_path}\")\n",
        "\n",
        "# CHANGE this to your real input file path\n",
        "input_file_path = 'en.txt'  # <-- CHANGE this!\n",
        "lines_per_split = 1000  # Split every 100,000 lines (adjust if needed)\n",
        "batch_size = 100  # Translate 100 lines at a time (adjust if needed)\n",
        "\n",
        "# Step 1: Split the big file\n",
        "print(\"üîµ Splitting large file...\")\n",
        "split_file(input_file_path, lines_per_file=lines_per_split)\n",
        "\n",
        "# Step 2: Translate each smaller file\n",
        "print(\"üîµ Translating each chunk...\")\n",
        "split_files = sorted(glob.glob('/content/split_files/*.txt'))\n",
        "\n",
        "for split_file_path in split_files:\n",
        "    print(f\"üîµ Now translating: {split_file_path}\")\n",
        "    translate_file(split_file_path, batch_size=batch_size)\n",
        "\n",
        "print(\"üèÜ All translations completed successfully!\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
